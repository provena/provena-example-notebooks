{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provena Toy Example\n",
    "\n",
    "This relies on some pre-registered components and is intended to show an example provenance enabled workflow by integrating with the provena APIs. \n",
    "\n",
    "The actual computation/validation has been stripped from the source notebook - but hopefully it will be clear where this can be reintroduced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provena workflow configuration setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small helper class which provides a config object for validation and\n",
    "# a loader function\n",
    "import example_workflow_config\n",
    "\n",
    "# This is a helper function for managing authentication with Provena\n",
    "from provenaclient import ProvenaClient, Config\n",
    "from provenaclient.auth import DeviceFlow\n",
    "from provenaclient.auth.implementations import OfflineFlow\n",
    "\n",
    "import json\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 01:01:44,694 - auth-logger - ERROR - The token used for refresh is invalid or has potentially expired. Something went wrong during token refresh. Status code: 400.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification URL: https://auth.dev.rrap-is.com/auth/realms/rrap/device?user_code=OWCZ-STKN\n",
      "User Code: OWCZ-STKN\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Provena config - replace with your Provena instance endpoints\n",
    "client_config = Config(\n",
    "    domain=\"dev.rrap-is.com\",\n",
    "    realm_name=\"rrap\"\n",
    ")\n",
    "\n",
    "\n",
    "offline_mode = False\n",
    "\n",
    "if offline_mode:\n",
    "    load_dotenv()\n",
    "    offline_token=os.getenv('PROVENA_API_TOKEN')\n",
    "    assert offline_token, \"Offline token must be present in .env file e.g. PROVENA_API_TOKEN=1234.\"\n",
    "    print(f\"Offline mode activated and token found in .env file.\")\n",
    "\n",
    "if not offline_mode:\n",
    "    auth = DeviceFlow(config=client_config,\n",
    "                    client_id=\"client-tools\")\n",
    "else:\n",
    "    auth = OfflineFlow(config=client_config, client_id=\"automated-access\", offline_token=offline_token)\n",
    "\n",
    "\n",
    "# Instantiate the client.\n",
    "client = ProvenaClient(config=client_config, auth=auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"inputs\": {\n",
      "    \"input_dataset\": \"10378.1/1904964\",\n",
      "    \"input_dataset_template\": \"10378.1/1905250\"\n",
      "  },\n",
      "  \"outputs\": {\n",
      "    \"output_dataset\": \"10378.1/1904961\",\n",
      "    \"output_dataset_template\": \"10378.1/1926245\"\n",
      "  },\n",
      "  \"associations\": {\n",
      "    \"person\": \"10378.1/1893843\",\n",
      "    \"organisation\": \"10378.1/1893860\"\n",
      "  },\n",
      "  \"workflow_configuration\": {\n",
      "    \"workflow_template\": \"10378.1/1905251\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Start by loading the config from the specified path \n",
    "\n",
    "# You will need to register: person, organisation, dataset template and model run workflow template and update the config\n",
    "\n",
    "# NOTE this could change from run to run - this holds all information required to run this model. \n",
    "config_path = \"configs/example_workflow3.json\"\n",
    "config = example_workflow_config.load_config(path=config_path)\n",
    "config.pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating registered Provena entities in config\n",
      "in loop.\n",
      "in here.\n",
      "Encountered exception while validating Dataset: id='10378.1/1904964'. Exception: Failed to fetch dataset with id 10378.1/1904964... Exception: 14 validation errors for RegistryFetchResponse\n",
      "item -> collection_format -> dataset_info -> created_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> collection_format -> dataset_info -> published_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 0 -> item -> collection_format -> dataset_info -> created_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 0 -> item -> collection_format -> dataset_info -> published_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 1 -> item -> collection_format -> dataset_info -> created_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 1 -> item -> collection_format -> dataset_info -> published_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 2 -> item -> collection_format -> dataset_info -> created_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 2 -> item -> collection_format -> dataset_info -> published_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 3 -> item -> collection_format -> dataset_info -> created_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 3 -> item -> collection_format -> dataset_info -> published_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 4 -> item -> collection_format -> dataset_info -> created_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 4 -> item -> collection_format -> dataset_info -> published_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 5 -> item -> collection_format -> dataset_info -> created_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error)\n",
      "item -> history -> 5 -> item -> collection_format -> dataset_info -> published_date\n",
      "  invalid type; expected date, string, bytes, int or float (type=type_error).\n",
      "Failed inputs validation.\n",
      "FAILED VALIDATION\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Workflow config validation exception occurred. See output above.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILED VALIDATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorkflow config validation exception occurred. See output above.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Workflow config validation exception occurred. See output above."
     ]
    }
   ],
   "source": [
    "# let's validate the workflow config - this fetches ALL items referenced in the\n",
    "# workflow json to ensure the items are valid \n",
    "\n",
    "\n",
    "valid = await config.validate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_entities(client)\n",
    "\n",
    "if not valid:\n",
    "    print(\"FAILED VALIDATION\")\n",
    "    raise Exception(\"Workflow config validation exception occurred. See output above.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model run integration\n",
    "Now that the validation of the workflow configuration (incl. registered entities) is complete - we can move into the example of running the model against this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Display Name is: Parth testing and Dataset ID is: 10378.1/1904964\n"
     ]
    }
   ],
   "source": [
    "# let's establish the paths of the input from the dataset\n",
    "def pprint_json(content) -> None:\n",
    "    print(json.dumps(content,indent=2))\n",
    "\n",
    "# fetch the dataset \n",
    "ds_id = config.inputs.input_dataset\n",
    "fetched_ds = await client.datastore.fetch_dataset(id=ds_id)\n",
    "\n",
    "print(\"Dataset Display Name is:\", fetched_ds.item.display_name, \"and Dataset ID is:\", fetched_ds.item.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# determine the external reposit path\n",
    "file_path = fetched_ds.item.collection_format.dataset_info.access_info.uri\n",
    "print(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated above, it is possible to retrieve the associated file path from the registered Dataset (assuming this info was included at registration time). Or the existing file path mechanism could continue being used.\n",
    "\n",
    "A similar approach works for the other inputs. Shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Dataset File Path: \": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "async def fetch_and_path(id: str):\n",
    "    dataset = await client.datastore.fetch_dataset(id=id)\n",
    "    path = dataset.item.collection_format.dataset_info.access_info.uri\n",
    "    return dataset, path\n",
    "\n",
    "# Dataset and Dataset file path.\n",
    "dataset, dataset_file_path_path = await fetch_and_path(id=config.inputs.input_dataset)\n",
    "\n",
    "pprint_json({\n",
    "   \"Dataset File Path: \" : dataset_file_path_path\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we wanted to use the data storage utilities of the Provena data store, we could register a reposited dataset, and use the dynamic credential generation to produce r or r/w credentials into that specific dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our fake model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to pretend to produce some output from this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fake hourly JYI calculation, took 5 seconds.\n"
     ]
    }
   ],
   "source": [
    "def fake_data_fetch(path: str) -> int:\n",
    "    # This method would take the path and return the data\n",
    "    return 0\n",
    "\n",
    "fake_temperature = fake_data_fetch(dataset_file_path_path)\n",
    "\n",
    "\n",
    "def fake_model(temperature: int) -> int:\n",
    "    # this model does some heavy lifting and takes 5 seconds to finish \n",
    "    time.sleep(5) \n",
    "    \n",
    "    return 0\n",
    "\n",
    "# let's run our model with the inputs \n",
    "\n",
    "# start timer\n",
    "start_time = int(time.time())\n",
    "\n",
    "# run the model \n",
    "fake_model_output = fake_model(\n",
    "    temperature=fake_temperature    \n",
    ")\n",
    "\n",
    "end_time = int(time.time())\n",
    "\n",
    "print(f\"Ran fake hourly JYI calculation, took {end_time - start_time} seconds.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ran the toy model, let's register a provenance record which records the model run, the inputs used, and the outputs produced.\n",
    "\n",
    "We need to think more about the output. \n",
    "\n",
    "There are two primary ways that Provena supports registering the results of a model run. \n",
    "\n",
    "1. Dynamically register a new Dataset and link to this dataset. This is the _preferred_ method as it creates a clear causal chain between the model and the output dataset.\n",
    "2. Use a deferred or defined resource in an output dataset template to register the outputs into an existing dataset. E.g. overwrite an existing file or contribute new files to an existing dataset. This method produces less structurally clear provenance chains and may obfuscate the history of data (if overwriting).\n",
    "\n",
    "We will show both methods. \n",
    "\n",
    "Model runs satisfy the following JSON schema\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"workflow_template_id\": \"string\",\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"dataset_template_id\": \"string\",\n",
    "      \"dataset_id\": \"string\",\n",
    "      \"dataset_type\": \"DATA_STORE\",\n",
    "      \"resources\": {\n",
    "        \"additionalProp1\": \"string\",\n",
    "        \"additionalProp2\": \"string\",\n",
    "        \"additionalProp3\": \"string\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"outputs\": [\n",
    "    {\n",
    "      \"dataset_template_id\": \"string\",\n",
    "      \"dataset_id\": \"string\",\n",
    "      \"dataset_type\": \"DATA_STORE\",\n",
    "      \"resources\": {\n",
    "        \"additionalProp1\": \"string\",\n",
    "        \"additionalProp2\": \"string\",\n",
    "        \"additionalProp3\": \"string\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"annotations\": {\n",
    "    \"additionalProp1\": \"string\",\n",
    "    \"additionalProp2\": \"string\",\n",
    "    \"additionalProp3\": \"string\"\n",
    "  },\n",
    "  \"description\": \"string\",\n",
    "  \"associations\": {\n",
    "    \"modeller_id\": \"string\",\n",
    "    \"requesting_organisation_id\": \"string\"\n",
    "  },\n",
    "  \"start_time\": 0,\n",
    "  \"end_time\": 0\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overwrite an existing output at a specified path\n",
    "\n",
    "Let's start with method 2) and overwrite a specified output. This dataset is pre-registered and is included in our config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemDataset(display_name='Parth testing', user_metadata=None, collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=datetime.date(2024, 6, 6), published_date=datetime.date(2024, 6, 6), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=None, temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904961/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904961/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, history=[HistoryEntry[DatasetDomainInfo](id=0, timestamp=1717633076, reason='Initial record creation', username='parth', item=DatasetDomainInfo(display_name='Parth testing', collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=datetime.date(2024, 6, 6), published_date=datetime.date(2024, 6, 6), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=None, temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904961/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904961/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, user_metadata=None))], id='10378.1/1904961', owner_username='parth', created_timestamp=1717633075, updated_timestamp=1717633076, item_category=<ItemCategory.ENTITY: 'ENTITY'>, item_subtype=<ItemSubType.DATASET: 'DATASET'>, record_type=<RecordType.COMPLETE_ITEM: 'COMPLETE_ITEM'>, workflow_links=WorkflowLinks(create_activity_workflow_id='3481da92-4972-455c-beab-0120b2c6d859', version_activity_workflow_id=None), versioning_info=VersioningInfo(previous_version=None, version=1, reason=None, next_version=None))\n"
     ]
    }
   ],
   "source": [
    "### Overwrite existing output\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "output_dataset_id = config.outputs.output_dataset\n",
    "\n",
    "# we can resolve the path using the same approach as above, or using existing\n",
    "# NBIC path structure\n",
    "\n",
    "output_ds, output_path = await fetch_and_path(id=output_dataset_id)\n",
    "\n",
    "pprint(output_ds.item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProvenaInterfaces.ProvenanceAPI import ModelRunRecord, TemplatedDataset, DatasetType, AssociationInfo\n",
    "from ProvenaInterfaces.AsyncJobAPI import JobStatus\n",
    "\n",
    "# Building the Model Run Payload.\n",
    "model_run_payload = ModelRunRecord(\n",
    "    workflow_template_id=config.workflow_configuration.workflow_template,\n",
    "    model_version = None, \n",
    "    inputs = [\n",
    "        TemplatedDataset(\n",
    "            dataset_template_id=config.inputs.input_dataset_template, \n",
    "            dataset_id=config.inputs.input_dataset,\n",
    "            dataset_type=DatasetType.DATA_STORE\n",
    "        )\n",
    "    ], \n",
    "    outputs=[\n",
    "        TemplatedDataset(\n",
    "            dataset_template_id=config.outputs.output_dataset_template, \n",
    "            dataset_id=config.outputs.output_dataset,\n",
    "            dataset_type=DatasetType.DATA_STORE\n",
    "        )\n",
    "    ], \n",
    "    annotations=None,\n",
    "    display_name=\"Notebook Model Run Testing\",\n",
    "    description=\"Standard Provena Model Run Example\",\n",
    "    study_id=None,\n",
    "    associations=AssociationInfo(\n",
    "        modeller_id=config.associations.person,\n",
    "        requesting_organisation_id=config.associations.organisation\n",
    "    ),\n",
    "    start_time=start_time,\n",
    "    end_time=end_time\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the model run \n",
    "model_run_register_result = await client.prov_api.register_model_run(model_run_payload=model_run_payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of registration success=True details='Job dispatched, monitor session ID using the job API to see progress.'\n",
      "Job Session ID e38c7adf-35fa-4eb3-9580-6b93d49d748c\n"
     ]
    }
   ],
   "source": [
    "# Check the response of the model run registration\n",
    "print(\"Status of registration\", model_run_register_result.status)\n",
    "print(\"Job Session ID\", model_run_register_result.session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting wait_for_entry_in_queue polling stage.\n",
      "Polling Job API. Wait time: 0sec out of 20sec.\n",
      "Running wait_for_entry_in_queue callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 2sec out of 20sec.\n",
      "Running wait_for_entry_in_queue callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "Finished wait_for_entry_in_queue polling stage.\n",
      "Starting wait_for_in_progress polling stage.\n",
      "Polling Job API. Wait time: 0sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 2sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 4sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 6sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 8sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 10sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 12sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 15sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 17sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 19sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 21sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 23sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 25sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 27sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 29sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 31sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 33sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 36sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 38sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 40sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 42sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 44sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 46sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 48sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in non pending state IN_PROGRESS.\n",
      "Finished wait_for_in_progress polling stage.\n",
      "Starting wait_for_completion polling stage.\n",
      "Polling Job API. Wait time: 0sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state session_id='e38c7adf-35fa-4eb3-9580-6b93d49d748c' created_timestamp=1720766695 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1720766689, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1720766694, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 2sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state session_id='e38c7adf-35fa-4eb3-9580-6b93d49d748c' created_timestamp=1720766695 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1720766689, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1720766694, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 4sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state session_id='e38c7adf-35fa-4eb3-9580-6b93d49d748c' created_timestamp=1720766695 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1720766689, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1720766694, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 7sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state session_id='e38c7adf-35fa-4eb3-9580-6b93d49d748c' created_timestamp=1720766695 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1720766689, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1720766694, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 9sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state session_id='e38c7adf-35fa-4eb3-9580-6b93d49d748c' created_timestamp=1720766695 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1720766689, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1720766694, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 11sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state session_id='e38c7adf-35fa-4eb3-9580-6b93d49d748c' created_timestamp=1720766695 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1720766689, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1720766694, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 13sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in state session_id='e38c7adf-35fa-4eb3-9580-6b93d49d748c' created_timestamp=1720766695 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1720766689, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1720766694, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 15sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: e38c7adf-35fa-4eb3-9580-6b93d49d748c.\n",
      "200OK response for user fetch of e38c7adf-35fa-4eb3-9580-6b93d49d748c in completed state.\n",
      "Finished wait_for_completion polling stage.\n",
      "\n",
      "Current job status: JobStatus.SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the job to see if it's complete. We will do this by polling the job_api\n",
    "job_result = await client.job_api.await_successful_job_completion(session_id=model_run_register_result.session_id)\n",
    "\n",
    "while job_result.status != JobStatus.SUCCEEDED: # Keep polling on this cell till this turns to \"SUCCEEDED\"\n",
    "    \n",
    "    job_result = await client.job_api.await_successful_job_completion(session_id=model_run_register_result.session_id)\n",
    "    pprint(job_result.result)\n",
    "    pprint(job_result.job_type)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Current job status:\", job_result.status) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prov_json\": \"{\\\"prefix\\\": {\\\"default\\\": \\\"http://hdl.handle.net/\\\"}, \\\"activity\\\": {\\\"10378.1/1926271\\\": {\\\"model_run/10378.1/1926271\\\": true, \\\"item_category\\\": \\\"ACTIVITY\\\", \\\"item_subtype\\\": \\\"MODEL_RUN\\\"}}, \\\"entity\\\": {\\\"10378.1/1904964\\\": {\\\"model_run/10378.1/1926271\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"DATASET\\\"}, \\\"10378.1/1904961\\\": {\\\"model_run/10378.1/1926271\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"DATASET\\\"}, \\\"10378.1/1905251\\\": {\\\"model_run/10378.1/1926271\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"MODEL_RUN_WORKFLOW_TEMPLATE\\\", \\\"prov:type\\\": {\\\"$\\\": \\\"prov:Collection\\\", \\\"type\\\": \\\"prov:QUALIFIED_NAME\\\"}}, \\\"10378.1/1905250\\\": {\\\"model_run/10378.1/1926271\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"DATASET_TEMPLATE\\\"}, \\\"10378.1/1926245\\\": {\\\"model_run/10378.1/1926271\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"DATASET_TEMPLATE\\\"}, \\\"10378.1/1924630\\\": {\\\"model_run/10378.1/1926271\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"MODEL\\\"}}, \\\"agent\\\": {\\\"10378.1/1893843\\\": {\\\"model_run/10378.1/1926271\\\": true, \\\"item_category\\\": \\\"AGENT\\\", \\\"item_subtype\\\": \\\"PERSON\\\"}, \\\"10378.1/1893860\\\": {\\\"model_run/10378.1/1926271\\\": true, \\\"item_category\\\": \\\"AGENT\\\", \\\"item_subtype\\\": \\\"ORGANISATION\\\"}}, \\\"used\\\": {\\\"_:id1\\\": {\\\"prov:activity\\\": \\\"10378.1/1926271\\\", \\\"prov:entity\\\": \\\"10378.1/1904964\\\"}, \\\"_:id3\\\": {\\\"prov:activity\\\": \\\"10378.1/1926271\\\", \\\"prov:entity\\\": \\\"10378.1/1924630\\\"}, \\\"_:id4\\\": {\\\"prov:activity\\\": \\\"10378.1/1926271\\\", \\\"prov:entity\\\": \\\"10378.1/1905251\\\"}}, \\\"wasGeneratedBy\\\": {\\\"_:id2\\\": {\\\"prov:entity\\\": \\\"10378.1/1904961\\\", \\\"prov:activity\\\": \\\"10378.1/1926271\\\"}}, \\\"wasAssociatedWith\\\": {\\\"_:id5\\\": {\\\"prov:activity\\\": \\\"10378.1/1926271\\\", \\\"prov:agent\\\": \\\"10378.1/1893843\\\"}, \\\"_:id6\\\": {\\\"prov:activity\\\": \\\"10378.1/1926271\\\", \\\"prov:agent\\\": \\\"10378.1/1893860\\\"}}, \\\"wasAttributedTo\\\": {\\\"_:id7\\\": {\\\"prov:entity\\\": \\\"10378.1/1904961\\\", \\\"prov:agent\\\": \\\"10378.1/1893843\\\"}}, \\\"hadMember\\\": {\\\"_:id8\\\": {\\\"prov:collection\\\": \\\"10378.1/1905251\\\", \\\"prov:entity\\\": \\\"10378.1/1905250\\\"}, \\\"_:id10\\\": {\\\"prov:collection\\\": \\\"10378.1/1905251\\\", \\\"prov:entity\\\": \\\"10378.1/1926245\\\"}, \\\"_:id12\\\": {\\\"prov:collection\\\": \\\"10378.1/1905251\\\", \\\"prov:entity\\\": \\\"10378.1/1924630\\\"}}, \\\"wasInfluencedBy\\\": {\\\"_:id9\\\": {\\\"prov:influencee\\\": \\\"10378.1/1904964\\\", \\\"prov:influencer\\\": \\\"10378.1/1905250\\\"}, \\\"_:id11\\\": {\\\"prov:influencee\\\": \\\"10378.1/1904961\\\", \\\"prov:influencer\\\": \\\"10378.1/1926245\\\"}}}\",\n",
      "  \"id\": \"10378.1/1926271\",\n",
      "  \"record\": {\n",
      "    \"outputs\": [\n",
      "      {\n",
      "        \"dataset_template_id\": \"10378.1/1926245\",\n",
      "        \"dataset_type\": \"DATA_STORE\",\n",
      "        \"dataset_id\": \"10378.1/1904961\"\n",
      "      }\n",
      "    ],\n",
      "    \"associations\": {\n",
      "      \"modeller_id\": \"10378.1/1893843\",\n",
      "      \"requesting_organisation_id\": \"10378.1/1893860\"\n",
      "    },\n",
      "    \"start_time\": 1720766689,\n",
      "    \"inputs\": [\n",
      "      {\n",
      "        \"dataset_template_id\": \"10378.1/1905250\",\n",
      "        \"dataset_type\": \"DATA_STORE\",\n",
      "        \"dataset_id\": \"10378.1/1904964\"\n",
      "      }\n",
      "    ],\n",
      "    \"end_time\": 1720766694,\n",
      "    \"description\": \"Standard Provena Model Run Example\",\n",
      "    \"workflow_template_id\": \"10378.1/1905251\",\n",
      "    \"display_name\": \"Notebook Model Run Testing\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "method_two_record_info = job_result.result[\"record\"]\n",
    "pprint_json(method_two_record_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick and dirty visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/graphviz/backend/execute.py:78\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: PosixPath('dot')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m filepath \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresources/prov_graph\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m filepath\u001b[38;5;241m.\u001b[39mwrite_text(src, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mgraphviz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/graphviz/backend/rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 326\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/graphviz/backend/execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "import prov.model as pm\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "import pathlib\n",
    "\n",
    "# pull out json serialisation from prov api\n",
    "prov_serialisation = method_two_record_info[\"prov_json\"]\n",
    "\n",
    "# parse into prov document\n",
    "document = pm.ProvDocument.deserialize(source=None, content=prov_serialisation)\n",
    "\n",
    "# render into file\n",
    "dot = prov_to_dot(document)\n",
    "src = dot.to_string()\n",
    "filepath = pathlib.Path('resources/prov_graph')\n",
    "filepath.write_text(src, encoding='ascii')\n",
    "\n",
    "graphviz.render('dot', 'png', filepath).replace('\\\\', '/')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registering a dataset during model run registration\n",
    "\n",
    "Now let's perform method 1) i.e. register a dataset during automated model run provenance registration.\n",
    "\n",
    "Datasets require the following fields.\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"associations\": {\n",
    "    \"organisation_id\": \"string\"\n",
    "  },\n",
    "  \"dataset_info\": {\n",
    "    \"name\": \"string\",\n",
    "    \"description\": \"string\",\n",
    "    \"access_info\": {\n",
    "      \"reposited\": true,\n",
    "      \"uri\": \"string\",\n",
    "      \"description\": \"string\"\n",
    "    },\n",
    "    \"publisher_id\": \"string\",\n",
    "    \"created_date\": \"2023-06-06\",\n",
    "    \"published_date\": \"2023-06-06\",\n",
    "    \"license\": \"string\",\n",
    "    \"preferred_citation\": \"string\",\n",
    "    \"keywords\": [\n",
    "      \"string\"\n",
    "    ],\n",
    "    \"version\": \"string\"\n",
    "  },\n",
    "  \"approvals\": {\n",
    "    \"ethics_registration\": {\n",
    "      \"relevant\": false,\n",
    "      \"obtained\": false\n",
    "    },\n",
    "    \"ethics_access\": {\n",
    "      \"relevant\": false,\n",
    "      \"obtained\": false\n",
    "    },\n",
    "    \"indigenous_knowledge\": {\n",
    "      \"relevant\": false,\n",
    "      \"obtained\": false\n",
    "    },\n",
    "    \"export_controls\": {\n",
    "      \"relevant\": false,\n",
    "      \"obtained\": false\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m default_license \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://gbrrestoration.github.io/rrap-mds-knowledge-hub/information-system/licenses.html#copyright-all-rights-reserved\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://example-bucket/test-data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m dataset_payload \u001b[38;5;241m=\u001b[39m CollectionFormat(\n\u001b[1;32m      8\u001b[0m         associations\u001b[38;5;241m=\u001b[39m CollectionFormatAssociations(\n\u001b[0;32m----> 9\u001b[0m             organisation_id\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39massociations\u001b[38;5;241m.\u001b[39morganisation,\n\u001b[1;32m     10\u001b[0m             data_custodian_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39massociations\u001b[38;5;241m.\u001b[39mperson,\n\u001b[1;32m     11\u001b[0m             point_of_contact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         ),\n\u001b[1;32m     13\u001b[0m         approvals\u001b[38;5;241m=\u001b[39mCollectionFormatApprovals(\n\u001b[1;32m     14\u001b[0m             ethics_registration\u001b[38;5;241m=\u001b[39mDatasetEthicsRegistrationCheck(),\n\u001b[1;32m     15\u001b[0m             ethics_access\u001b[38;5;241m=\u001b[39mDatasetEthicsAccessCheck(),\n\u001b[1;32m     16\u001b[0m             indigenous_knowledge\u001b[38;5;241m=\u001b[39mIndigenousKnowledgeCheck(),\n\u001b[1;32m     17\u001b[0m             export_controls\u001b[38;5;241m=\u001b[39mExportControls()\n\u001b[1;32m     18\u001b[0m         ), \n\u001b[1;32m     19\u001b[0m         dataset_info\u001b[38;5;241m=\u001b[39mCollectionFormatDatasetInfo(\n\u001b[1;32m     20\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNotebook Example workflow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m             description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample workflow output (Standard Provena),Generated in automated provenance workflow registration. Externally reposited\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     22\u001b[0m             access_info\u001b[38;5;241m=\u001b[39mAccessInfo(reposited\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, uri \u001b[38;5;241m=\u001b[39m output_path, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file is stored in the Example storage at the specified path\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     23\u001b[0m             publisher_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39massociations\u001b[38;5;241m.\u001b[39morganisation,\n\u001b[1;32m     24\u001b[0m             created_date\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-06-06\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m             published_date\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-06-06\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m             license\u001b[38;5;241m=\u001b[39mdefault_license,\n\u001b[1;32m     27\u001b[0m             purpose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     28\u001b[0m             rights_holder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m             usage_limitations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m             temporal_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m             formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m             keywords\u001b[38;5;241m=\u001b[39m[ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJYI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     33\u001b[0m             user_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m             version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m         \n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m register_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mmint_dataset(dataset_mint_info\u001b[38;5;241m=\u001b[39mdataset_payload)\n\u001b[1;32m     41\u001b[0m output_dataset_id \u001b[38;5;241m=\u001b[39m register_dataset\u001b[38;5;241m.\u001b[39mhandle\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "from ProvenaInterfaces.RegistryAPI import *\n",
    "from ProvenaInterfaces.DataStoreAPI import CollectionFormat\n",
    "\n",
    "default_license = \"https://gbrrestoration.github.io/rrap-mds-knowledge-hub/information-system/licenses.html#copyright-all-rights-reserved\"\n",
    "output_path = \"s3://example-bucket/test-data.csv\"\n",
    "\n",
    "dataset_payload = CollectionFormat(\n",
    "        associations= CollectionFormatAssociations(\n",
    "            organisation_id=config.associations.organisation,\n",
    "            data_custodian_id=config.associations.person,\n",
    "            point_of_contact=None\n",
    "        ),\n",
    "        approvals=CollectionFormatApprovals(\n",
    "            ethics_registration=DatasetEthicsRegistrationCheck(),\n",
    "            ethics_access=DatasetEthicsAccessCheck(),\n",
    "            indigenous_knowledge=IndigenousKnowledgeCheck(),\n",
    "            export_controls=ExportControls()\n",
    "        ), \n",
    "        dataset_info=CollectionFormatDatasetInfo(\n",
    "            name=\"Notebook Example workflow\",\n",
    "            description = \"Example workflow output (Standard Provena),Generated in automated provenance workflow registration. Externally reposited\", \n",
    "            access_info=AccessInfo(reposited=False, uri = output_path, description=\"The file is stored in the Example storage at the specified path\"),\n",
    "            publisher_id=config.associations.organisation,\n",
    "            created_date= CreatedDate(relevant = True, value = \"\"),\n",
    "            created_date= PublishedDate(relevant = True, value = \"\"),\n",
    "            license=default_license,\n",
    "            purpose = None, \n",
    "            rights_holder=None,\n",
    "            usage_limitations=None,\n",
    "            temporal_info=None,\n",
    "            formats=None,\n",
    "            keywords=[ \"JYI\",\"Example\"],\n",
    "            user_metadata=None,\n",
    "            version=None\n",
    "        )\n",
    "        \n",
    ")\n",
    "\n",
    "register_dataset = await client.datastore.mint_dataset(dataset_mint_info=dataset_payload)\n",
    "\n",
    "output_dataset_id = register_dataset.handle\n",
    "\n",
    "print(f\"Minted new dataset successfully with handle {output_dataset_id}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have dynamically generated an output dataset. \n",
    "\n",
    "We could choose to actually upload the files to this dataset using the dynamic s3 credential generation. \n",
    "\n",
    "However, in the NBIC workflows we just want to refer to the existing storage location - so we use the externally reposited option as above to specify this path. \n",
    "\n",
    "Now we can use the same model run payload, replacing the output dataset with the dynamically generated output above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model run\n",
      "Token validation failed due to error: Signature has expired.\n",
      "Refreshing using refresh token\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Non 200 status code in response: 404. Response: {\"detail\":\"Not Found\"}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(url\u001b[39m=\u001b[39mendpoint, json\u001b[39m=\u001b[39mpayload, auth\u001b[39m=\u001b[39mget_auth())\n\u001b[1;32m     13\u001b[0m \u001b[39m# use helper function to check response\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m registry\u001b[39m.\u001b[39;49mcheck_response(response\u001b[39m=\u001b[39;49mresponse, status_check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/srv/repo/github/provena/provena-example-notebooks/python/registry.py:9\u001b[0m, in \u001b[0;36mcheck_response\u001b[0;34m(response, status_check)\u001b[0m\n\u001b[1;32m      7\u001b[0m status_code \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code\n\u001b[1;32m      8\u001b[0m \u001b[39mif\u001b[39;00m (status_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m     10\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNon 200 status code in response: \u001b[39m\u001b[39m{\u001b[39;00mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m. Response: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mtext\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m status_check:\n\u001b[1;32m     12\u001b[0m     status_object \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mException\u001b[0m: Non 200 status code in response: 404. Response: {\"detail\":\"Not Found\"}."
     ]
    }
   ],
   "source": [
    "model_run_payload.outputs[0].dataset_id = output_dataset_id\n",
    "\n",
    "## Registering the model run \n",
    "payload = model_run_payload\n",
    "print(\"Registering model run\")\n",
    "model_run_register_response = await client.prov_api.register_model_run(model_run_payload=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the job to see if it's complete. We will do this by polling the job_api\n",
    "job_result = await client.job_api.await_successful_job_completion(session_id=model_run_register_result.session_id)\n",
    "\n",
    "while job_result.status != JobStatus.SUCCEEDED: # Keep polling on this cell till this turns to \"SUCCEEDED\"\n",
    "    \n",
    "    job_result = await client.job_api.await_successful_job_completion(session_id=model_run_register_result.session_id)\n",
    "    pprint(job_result.result)\n",
    "    pprint(job_result.job_type)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Current job status:\", job_result.status) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_one_record_info = job_result.result[\"record\"]\n",
    "\n",
    "# pull out json serialisation from prov api\n",
    "prov_serialisation = method_one_record_info[\"prov_json\"]\n",
    "\n",
    "# parse into prov document\n",
    "document = pm.ProvDocument.deserialize(source=None, content=prov_serialisation)\n",
    "\n",
    "# render into file\n",
    "dot = prov_to_dot(document)\n",
    "name = \"resources/prov_graph\"\n",
    "\n",
    "dot.write_png(name + '.png')\n",
    "Image(name + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating download and upload\n",
    "\n",
    "If, unlike in the above demonstration, we wanted to upload and download files, it is easy to use the Provena APIs to automate this process. \n",
    "\n",
    "The high level steps are:\n",
    "\n",
    "1. Identify or register the dataset\n",
    "2. Use the API to generate credentials to r/w to the dataset\n",
    "3. Use these credentials in your S3 client of choice to r/w data\n",
    "\n",
    "We will demonstrate these steps below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering dataset\n",
      "Token validation failed due to error: Signature has expired.\n",
      "Refreshing using refresh token\n",
      "\n",
      "Minted new dataset successfully with handle 10378.1/1803642.\n"
     ]
    }
   ],
   "source": [
    "# First, let's register a dataset\n",
    "\n",
    "default_license = \"https://gbrrestoration.github.io/rrap-mds-knowledge-hub/information-system/licenses.html#copyright-all-rights-reserved\"\n",
    "dataset_payload = CollectionFormat(\n",
    "    associations=CollectionFormatAssociations(\n",
    "        organisation_id=config.associations.organisation,\n",
    "        data_custodian_id=None,\n",
    "        point_of_contact=None\n",
    "    ),\n",
    "    approvals=CollectionFormatApprovals(\n",
    "        ethics_registration=DatasetEthicsRegistrationCheck(\n",
    "            relevant=False,\n",
    "            obtained=False\n",
    "        ),\n",
    "        ethics_access=DatasetEthicsAccessCheck(\n",
    "            relevant=False,\n",
    "            obtained=False\n",
    "        ),\n",
    "        indigenous_knowledge=IndigenousKnowledgeCheck(\n",
    "            relevant=False,\n",
    "            obtained=False\n",
    "        ),\n",
    "        export_controls=ExportControls(\n",
    "            relevant=False,\n",
    "            obtained=False\n",
    "        )\n",
    "    ),\n",
    "    dataset_info=CollectionFormatDatasetInfo(\n",
    "        name=\"Demonstration upload download dataset\",\n",
    "        description=\"This dataset is used to demonstrate automated upload and download of files.\",\n",
    "        access_info=AccessInfo(\n",
    "            reposited=True,\n",
    "            uri=None,\n",
    "            description=None\n",
    "        ),\n",
    "        publisher_id=config.associations.organisation,\n",
    "        created_date=\"2023-06-06\",\n",
    "        published_date=\"2023-06-06\",\n",
    "        license=default_license,\n",
    "        purpose=None,\n",
    "        rights_holder=None,\n",
    "        usage_limitations=None,\n",
    "        temporal_info=None,\n",
    "        formats=None,\n",
    "        keywords=[\"JYI\", \"Example\"],\n",
    "        user_metadata=None,\n",
    "        version=None\n",
    "    )\n",
    ")\n",
    "\n",
    "# send off request\n",
    "print(\"Registering dataset\")\n",
    "dataset_mint_response = await client.datastore.mint_dataset(dataset_mint_info=dataset_payload)\n",
    "\n",
    "dataset_id = dataset_mint_response.handle\n",
    "\n",
    "print(f\"Minted new dataset successfully with handle {dataset_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: restored-dev-dev-rrap-storage-bucket-11102022-11102022, Path: datasets/10378-1-1803642/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_id = dataset_mint_response.handle\n",
    "bucket_name = dataset_mint_response.s3_location.bucket_name\n",
    "bucket_path = dataset_mint_response.s3_location.path\n",
    "\n",
    "print(f\"Name: {bucket_name}, Path: {bucket_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can use the built in download/upload methods, or we can spin up our own\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with TemporaryDirectory() as upload_dir: \n",
    "    # Assume we have these files in our local system initially\n",
    "    local_files = ['document.txt', 'photo.jpg', 'data.csv']\n",
    "\n",
    "    # Create the files locally to simulate existing files\n",
    "    for file_name in local_files:\n",
    "        with open(os.path.join(upload_dir, file_name), 'w') as file:\n",
    "            file.write(f\"This is the content of {file_name}.\")\n",
    "\n",
    "    # Upload all files in dir\n",
    "    print(\"Uploading files...\")\n",
    "    upload(upload_dir)\n",
    "    print()\n",
    "    \n",
    "    # Download and check into temp dir\n",
    "    with TemporaryDirectory() as download_dir: \n",
    "        print(\"Downloading files...\")\n",
    "        await client.datastore.io.download_all_files(destination_directory=dest)\n",
    "        print()\n",
    "        \n",
    "        print(\"Resulting files...\")\n",
    "        files = os.listdir(download_dir)\n",
    "        for f in files:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading files...\n",
      "Found dataset: Demonstration upload download dataset.\n",
      "\n",
      "Attempting to upload files to /tmp/tmp9jt_mong\n",
      "Upload complete.\n",
      "\n",
      "Downloading files...\n",
      "Found dataset: Demonstration upload download dataset.\n",
      "\n",
      "Attempting to download files to /tmp/tmp61k95v21\n",
      "Download complete.\n",
      "\n",
      "Resulting files...\n",
      "photo.jpg\n",
      "metadata.json\n",
      "document.txt\n",
      "data.csv\n"
     ]
    }
   ],
   "source": [
    "# Built in upload\n",
    "\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "# make some helper funcs\n",
    "def upload(path: str):\n",
    "    ProvenaRW.upload(data_store_api_endpoint=data_store_endpoint, handle=dataset_id, auth=get_auth(), source_dir=path)\n",
    "\n",
    "def download(dest: str):\n",
    "    ProvenaRW.download(data_store_api_endpoint=data_store_endpoint,handle=dataset_id, auth=get_auth(),download_path=dest)\n",
    "   \n",
    "  \n",
    "with TemporaryDirectory() as upload_dir: \n",
    "    # Assume we have these files in our local system initially\n",
    "    local_files = ['document.txt', 'photo.jpg', 'data.csv']\n",
    "\n",
    "    # Create the files locally to simulate existing files\n",
    "    for file_name in local_files:\n",
    "        with open(os.path.join(upload_dir, file_name), 'w') as file:\n",
    "            file.write(f\"This is the content of {file_name}.\")\n",
    "\n",
    "    # Upload all files in dir\n",
    "    print(\"Uploading files...\")\n",
    "    upload(upload_dir)\n",
    "    print()\n",
    "    \n",
    "    # Download and check into temp dir\n",
    "    with TemporaryDirectory() as download_dir: \n",
    "        print(\"Downloading files...\")\n",
    "        download(download_dir)\n",
    "        print()\n",
    "        \n",
    "        print(\"Resulting files...\")\n",
    "        files = os.listdir(download_dir)\n",
    "        for f in files:\n",
    "            print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading files using S3 SDK...\n",
      "\n",
      "Downloading files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset: Demonstration upload download dataset.\n",
      "\n",
      "Attempting to download files to /tmp/tmpxcg7ft6w\n",
      "Download complete.\n",
      "\n",
      "Resulting files...\n",
      "metadata.json\n",
      "data2.csv\n",
      "photo2.jpg\n",
      "document2.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Or we can use AWS S3 SDK directly, for example\n",
    "import boto3 \n",
    "\n",
    "# This will mess up constructor - remove for now\n",
    "filtered_creds = {k : v for k,v in write_creds.items() if k != \"expiry\"}\n",
    "\n",
    "# create a session\n",
    "s3 = boto3.client('s3', **filtered_creds)\n",
    "\n",
    "# now lets demonstrate uploading/downloading directly\n",
    "with TemporaryDirectory() as upload_dir: \n",
    "    # Assume we have these files in our local system initially\n",
    "    local_files = ['document2.txt', 'photo2.jpg', 'data2.csv']\n",
    "\n",
    "    # Create the files locally to simulate existing files\n",
    "    for file_name in local_files:\n",
    "        with open(os.path.join(upload_dir, file_name), 'w') as file:\n",
    "            file.write(f\"This is the content of {file_name}.\")\n",
    "\n",
    "    # Upload all files in dir\n",
    "    print(\"Uploading files using S3 SDK...\")\n",
    "    for f in local_files:\n",
    "        s3.upload_file(os.path.join(upload_dir, f),bucket_name,bucket_path +  f)\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    # Download and check into temp dir\n",
    "    with TemporaryDirectory() as download_dir: \n",
    "        print(\"Downloading files...\")\n",
    "        download(download_dir)\n",
    "        print()\n",
    "        \n",
    "        print(\"Resulting files...\")\n",
    "        files = os.listdir(download_dir)\n",
    "        for f in files:\n",
    "            print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
