{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provena Toy Example\n",
    "\n",
    "This relies on some pre-registered components and is intended to show an example provenance enabled workflow by integrating with the provena APIs. \n",
    "\n",
    "The actual computation/validation has been stripped from the source notebook - but hopefully it will be clear where this can be reintroduced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provena workflow configuration setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small helper class which provides a config object for validation and\n",
    "# a loader function\n",
    "import example_workflow_config\n",
    "\n",
    "# This is a helper function for managing authentication with Provena\n",
    "from provenaclient import ProvenaClient, Config\n",
    "from provenaclient.auth import DeviceFlow\n",
    "from provenaclient.auth.implementations import OfflineFlow\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Provena config - replace with your Provena instance endpoints\n",
    "client_config = Config(\n",
    "    domain=\"dev.rrap-is.com\",\n",
    "    realm_name=\"rrap\"\n",
    ")\n",
    "\n",
    "\n",
    "offline_mode = False\n",
    "\n",
    "if offline_mode:\n",
    "    load_dotenv()\n",
    "    offline_token=os.getenv('PROVENA_API_TOKEN')\n",
    "    assert offline_token, \"Offline token must be present in .env file e.g. PROVENA_API_TOKEN=1234.\"\n",
    "    print(f\"Offline mode activated and token found in .env file.\")\n",
    "\n",
    "if not offline_mode:\n",
    "    auth = DeviceFlow(config=client_config,\n",
    "                    client_id=\"client-tools\")\n",
    "else:\n",
    "    auth = OfflineFlow(config=client_config, client_id=\"automated-access\", offline_token=offline_token)\n",
    "\n",
    "\n",
    "# Instantiate the client.\n",
    "client = ProvenaClient(config=client_config, auth=auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"inputs\": {\n",
      "    \"input_dataset\": \"10378.1/1904964\",\n",
      "    \"input_dataset_template\": \"10378.1/1905250\"\n",
      "  },\n",
      "  \"outputs\": {\n",
      "    \"output_dataset\": \"10378.1/1904961\",\n",
      "    \"output_dataset_template\": \"10378.1/1926245\"\n",
      "  },\n",
      "  \"associations\": {\n",
      "    \"person\": \"10378.1/1893843\",\n",
      "    \"organisation\": \"10378.1/1893860\"\n",
      "  },\n",
      "  \"workflow_configuration\": {\n",
      "    \"workflow_template\": \"10378.1/1905251\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Start by loading the config from the specified path \n",
    "\n",
    "# You will need to register: person, organisation, dataset template and model run workflow template and update the config\n",
    "\n",
    "# NOTE this could change from run to run - this holds all information required to run this model. \n",
    "config_path = \"configs/example_workflow3.json\"\n",
    "config = example_workflow_config.load_config(path=config_path)\n",
    "config.pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating registered Provena entities in config\n",
      "in loop.\n",
      "in here.\n",
      "status=Status(success=True, details=\"Successfully fetched data for handle '10378.1/1904964'\") item=ItemDataset(display_name='Parth testing', user_metadata=None, collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=CreatedDate(relevant=True, value=datetime.date(2024, 6, 6)), published_date=PublishedDate(relevant=True, value=datetime.date(2024, 6, 6)), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=CollectionFormatSpatialInfo(coverage=None, resolution=None, extent='SRID=4326;POINT(-44.3 60.1)'), temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904964/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904964/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, history=[HistoryEntry[DatasetDomainInfo](id=5, timestamp=1718239752, reason='s', username='ross', item=DatasetDomainInfo(display_name='Parth testing', collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=CreatedDate(relevant=True, value=datetime.date(2024, 6, 6)), published_date=PublishedDate(relevant=True, value=datetime.date(2024, 6, 6)), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=CollectionFormatSpatialInfo(coverage=None, resolution=None, extent='SRID=4326;POINT(-44.3 60.1)'), temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904964/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904964/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, user_metadata=None)), HistoryEntry[DatasetDomainInfo](id=4, timestamp=1718239743, reason='ss', username='ross', item=DatasetDomainInfo(display_name='Parth testing', collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=CreatedDate(relevant=True, value=datetime.date(2024, 6, 6)), published_date=PublishedDate(relevant=True, value=datetime.date(2024, 6, 6)), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=CollectionFormatSpatialInfo(coverage='SRID=4326;POINT(-44.3 60.1)', resolution=None, extent='SRID=4326;POINT(-44.3 60.1)'), temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904964/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904964/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, user_metadata=None)), HistoryEntry[DatasetDomainInfo](id=3, timestamp=1718239729, reason='cccc', username='ross', item=DatasetDomainInfo(display_name='Parth testing', collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=CreatedDate(relevant=True, value=datetime.date(2024, 6, 6)), published_date=PublishedDate(relevant=True, value=datetime.date(2024, 6, 6)), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=CollectionFormatSpatialInfo(coverage='SRID=4326;POINT(-44.3 60.1)', resolution=None, extent=None), temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904964/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904964/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, user_metadata=None)), HistoryEntry[DatasetDomainInfo](id=2, timestamp=1718237215, reason='v', username='ross', item=DatasetDomainInfo(display_name='Parth testing', collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=CreatedDate(relevant=True, value=datetime.date(2024, 6, 6)), published_date=PublishedDate(relevant=True, value=datetime.date(2024, 6, 6)), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=CollectionFormatSpatialInfo(coverage=None, resolution=None, extent='SRID=4326;POINT(-44.3 60.1)'), temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904964/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904964/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, user_metadata=None)), HistoryEntry[DatasetDomainInfo](id=1, timestamp=1718237197, reason='x', username='ross', item=DatasetDomainInfo(display_name='Parth testing', collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=CreatedDate(relevant=True, value=datetime.date(2024, 6, 6)), published_date=PublishedDate(relevant=True, value=datetime.date(2024, 6, 6)), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=CollectionFormatSpatialInfo(coverage='SRID=4326;POINT(-44.3 60.1)', resolution=None, extent=None), temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904964/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904964/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, user_metadata=None)), HistoryEntry[DatasetDomainInfo](id=0, timestamp=1717633792, reason='Initial record creation', username='parth', item=DatasetDomainInfo(display_name='Parth testing', collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=CreatedDate(relevant=True, value=datetime.date(2024, 6, 6)), published_date=PublishedDate(relevant=True, value=datetime.date(2024, 6, 6)), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=None, temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904964/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904964/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, user_metadata=None))], id='10378.1/1904964', owner_username='parth', created_timestamp=1717633791, updated_timestamp=1718239752, item_category=<ItemCategory.ENTITY: 'ENTITY'>, item_subtype=<ItemSubType.DATASET: 'DATASET'>, record_type=<RecordType.COMPLETE_ITEM: 'COMPLETE_ITEM'>, workflow_links=WorkflowLinks(create_activity_workflow_id='560d75cc-7dbf-49b2-adad-4aa6dc1637db', version_activity_workflow_id=None), versioning_info=VersioningInfo(previous_version=None, version=1, reason=None, next_version=None)) roles=['metadata-read', 'metadata-write', 'admin', 'dataset-data-read', 'dataset-data-write'] locked=False\n",
      "Validating registered output datasets...\n",
      "Validating registered associations...\n",
      "Validating registered associations...\n",
      "Validation successful...\n"
     ]
    }
   ],
   "source": [
    "# let's validate the workflow config - this fetches ALL items referenced in the\n",
    "# workflow json to ensure the items are valid \n",
    "\n",
    "\n",
    "valid = await config.validate_entities(client)\n",
    "\n",
    "if not valid:\n",
    "    print(\"FAILED VALIDATION\")\n",
    "    raise Exception(\"Workflow config validation exception occurred. See output above.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model run integration\n",
    "Now that the validation of the workflow configuration (incl. registered entities) is complete - we can move into the example of running the model against this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Display Name is: Parth testing and Dataset ID is: 10378.1/1904964\n"
     ]
    }
   ],
   "source": [
    "# let's establish the paths of the input from the dataset\n",
    "def pprint_json(content) -> None:\n",
    "    print(json.dumps(content,indent=2))\n",
    "\n",
    "# fetch the dataset \n",
    "ds_id = config.inputs.input_dataset\n",
    "fetched_ds = await client.datastore.fetch_dataset(id=ds_id)\n",
    "\n",
    "print(\"Dataset Display Name is:\", fetched_ds.item.display_name, \"and Dataset ID is:\", fetched_ds.item.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# determine the external reposit path\n",
    "file_path = fetched_ds.item.collection_format.dataset_info.access_info.uri\n",
    "print(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated above, it is possible to retrieve the associated file path from the registered Dataset (assuming this info was included at registration time). Or the existing file path mechanism could continue being used.\n",
    "\n",
    "A similar approach works for the other inputs. Shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Dataset File Path: \": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "async def fetch_and_path(id: str):\n",
    "    dataset = await client.datastore.fetch_dataset(id=id)\n",
    "    path = dataset.item.collection_format.dataset_info.access_info.uri\n",
    "    return dataset, path\n",
    "\n",
    "# Dataset and Dataset file path.\n",
    "dataset, dataset_file_path_path = await fetch_and_path(id=config.inputs.input_dataset)\n",
    "\n",
    "pprint_json({\n",
    "   \"Dataset File Path: \" : dataset_file_path_path\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we wanted to use the data storage utilities of the Provena data store, we could register a reposited dataset, and use the dynamic credential generation to produce r or r/w credentials into that specific dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our fake model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to pretend to produce some output from this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fake hourly JYI calculation, took 5 seconds.\n"
     ]
    }
   ],
   "source": [
    "def fake_data_fetch(path: str) -> int:\n",
    "    # This method would take the path and return the data\n",
    "    return 0\n",
    "\n",
    "fake_temperature = fake_data_fetch(dataset_file_path_path)\n",
    "\n",
    "\n",
    "def fake_model(temperature: int) -> int:\n",
    "    # this model does some heavy lifting and takes 5 seconds to finish \n",
    "    time.sleep(5) \n",
    "    \n",
    "    return 0\n",
    "\n",
    "# let's run our model with the inputs \n",
    "\n",
    "# start timer\n",
    "start_time = int(time.time())\n",
    "\n",
    "# run the model \n",
    "fake_model_output = fake_model(\n",
    "    temperature=fake_temperature    \n",
    ")\n",
    "\n",
    "end_time = int(time.time())\n",
    "\n",
    "print(f\"Ran fake hourly JYI calculation, took {end_time - start_time} seconds.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ran the toy model, let's register a provenance record which records the model run, the inputs used, and the outputs produced.\n",
    "\n",
    "We need to think more about the output. \n",
    "\n",
    "There are two primary ways that Provena supports registering the results of a model run. \n",
    "\n",
    "1. Dynamically register a new Dataset and link to this dataset. This is the _preferred_ method as it creates a clear causal chain between the model and the output dataset.\n",
    "2. Use a deferred or defined resource in an output dataset template to register the outputs into an existing dataset. E.g. overwrite an existing file or contribute new files to an existing dataset. This method produces less structurally clear provenance chains and may obfuscate the history of data (if overwriting).\n",
    "\n",
    "We will show both methods. \n",
    "\n",
    "Model runs satisfy the following JSON schema\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"workflow_template_id\": \"string\",\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"dataset_template_id\": \"string\",\n",
    "      \"dataset_id\": \"string\",\n",
    "      \"dataset_type\": \"DATA_STORE\",\n",
    "      \"resources\": {\n",
    "        \"additionalProp1\": \"string\",\n",
    "        \"additionalProp2\": \"string\",\n",
    "        \"additionalProp3\": \"string\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"outputs\": [\n",
    "    {\n",
    "      \"dataset_template_id\": \"string\",\n",
    "      \"dataset_id\": \"string\",\n",
    "      \"dataset_type\": \"DATA_STORE\",\n",
    "      \"resources\": {\n",
    "        \"additionalProp1\": \"string\",\n",
    "        \"additionalProp2\": \"string\",\n",
    "        \"additionalProp3\": \"string\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"annotations\": {\n",
    "    \"additionalProp1\": \"string\",\n",
    "    \"additionalProp2\": \"string\",\n",
    "    \"additionalProp3\": \"string\"\n",
    "  },\n",
    "  \"description\": \"string\",\n",
    "  \"associations\": {\n",
    "    \"modeller_id\": \"string\",\n",
    "    \"requesting_organisation_id\": \"string\"\n",
    "  },\n",
    "  \"start_time\": 0,\n",
    "  \"end_time\": 0\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overwrite an existing output at a specified path\n",
    "\n",
    "Let's start with method 2) and overwrite a specified output. This dataset is pre-registered and is included in our config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemDataset(display_name='Parth testing', user_metadata=None, collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=CreatedDate(relevant=True, value=datetime.date(2024, 6, 6)), published_date=PublishedDate(relevant=True, value=datetime.date(2024, 6, 6)), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=None, temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904961/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904961/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, history=[HistoryEntry[DatasetDomainInfo](id=0, timestamp=1717633076, reason='Initial record creation', username='parth', item=DatasetDomainInfo(display_name='Parth testing', collection_format=CollectionFormat(associations=CollectionFormatAssociations(organisation_id='10378.1/1893860', data_custodian_id='10378.1/1893843', point_of_contact=None), approvals=CollectionFormatApprovals(ethics_registration=DatasetEthicsRegistrationCheck(relevant=False, obtained=False), ethics_access=DatasetEthicsAccessCheck(relevant=False, obtained=False), indigenous_knowledge=IndigenousKnowledgeCheck(relevant=False, obtained=False), export_controls=ExportControls(relevant=False, obtained=False)), dataset_info=CollectionFormatDatasetInfo(name='Parth testing', description='testing dataset', access_info=AccessInfo(reposited=True, uri=None, description=None), publisher_id='10378.1/1893860', created_date=CreatedDate(relevant=True, value=datetime.date(2024, 6, 6)), published_date=PublishedDate(relevant=True, value=datetime.date(2024, 6, 6)), license=AnyHttpUrl('https://www.google.com', ), purpose=None, rights_holder=None, usage_limitations=None, preferred_citation=None, spatial_info=None, temporal_info=None, formats=None, keywords=None, user_metadata=None, version=None)), s3=S3Location(bucket_name='restored-dev-dev-rrap-storage-bucket-11102022-11102022', path='datasets/10378-1-1904961/', s3_uri='s3://restored-dev-dev-rrap-storage-bucket-11102022-11102022/datasets/10378-1-1904961/'), release_history=[], release_status=<ReleasedStatus.NOT_RELEASED: 'NOT_RELEASED'>, release_approver=None, release_timestamp=None, access_info_uri=None, user_metadata=None))], id='10378.1/1904961', owner_username='parth', created_timestamp=1717633075, updated_timestamp=1717633076, item_category=<ItemCategory.ENTITY: 'ENTITY'>, item_subtype=<ItemSubType.DATASET: 'DATASET'>, record_type=<RecordType.COMPLETE_ITEM: 'COMPLETE_ITEM'>, workflow_links=WorkflowLinks(create_activity_workflow_id='3481da92-4972-455c-beab-0120b2c6d859', version_activity_workflow_id=None), versioning_info=VersioningInfo(previous_version=None, version=1, reason=None, next_version=None))\n"
     ]
    }
   ],
   "source": [
    "### Overwrite existing output\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "output_dataset_id = config.outputs.output_dataset\n",
    "\n",
    "# we can resolve the path using the same approach as above, or using existing\n",
    "# NBIC path structure\n",
    "\n",
    "output_ds, output_path = await fetch_and_path(id=output_dataset_id)\n",
    "\n",
    "pprint(output_ds.item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProvenaInterfaces.ProvenanceAPI import ModelRunRecord, TemplatedDataset, DatasetType, AssociationInfo\n",
    "from ProvenaInterfaces.AsyncJobAPI import JobStatus\n",
    "\n",
    "# Building the Model Run Payload.\n",
    "model_run_payload = ModelRunRecord(\n",
    "    workflow_template_id=config.workflow_configuration.workflow_template,\n",
    "    model_version = None, \n",
    "    inputs = [\n",
    "        TemplatedDataset(\n",
    "            dataset_template_id=config.inputs.input_dataset_template, \n",
    "            dataset_id=config.inputs.input_dataset,\n",
    "            dataset_type=DatasetType.DATA_STORE\n",
    "        )\n",
    "    ], \n",
    "    outputs=[\n",
    "        TemplatedDataset(\n",
    "            dataset_template_id=config.outputs.output_dataset_template, \n",
    "            dataset_id=config.outputs.output_dataset,\n",
    "            dataset_type=DatasetType.DATA_STORE\n",
    "        )\n",
    "    ], \n",
    "    annotations=None,\n",
    "    display_name=\"Notebook Model Run Testing\",\n",
    "    description=\"Standard Provena Model Run Example\",\n",
    "    study_id=None,\n",
    "    associations=AssociationInfo(\n",
    "        modeller_id=config.associations.person,\n",
    "        requesting_organisation_id=config.associations.organisation\n",
    "    ),\n",
    "    start_time=start_time,\n",
    "    end_time=end_time\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the model run \n",
    "model_run_register_result = await client.prov_api.register_model_run(model_run_payload=model_run_payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of registration success=True details='Job dispatched, monitor session ID using the job API to see progress.'\n",
      "Job Session ID 7f517cb7-80d3-436e-8a46-3044aaec4e2a\n"
     ]
    }
   ],
   "source": [
    "# Check the response of the model run registration\n",
    "print(\"Status of registration\", model_run_register_result.status)\n",
    "print(\"Job Session ID\", model_run_register_result.session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting wait_for_entry_in_queue polling stage.\n",
      "Polling Job API. Wait time: 0sec out of 20sec.\n",
      "Running wait_for_entry_in_queue callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 2sec out of 20sec.\n",
      "Running wait_for_entry_in_queue callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "Finished wait_for_entry_in_queue polling stage.\n",
      "Starting wait_for_in_progress polling stage.\n",
      "Polling Job API. Wait time: 0sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 2sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 4sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 6sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 8sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 10sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 12sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 14sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 16sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 19sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 21sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 23sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 25sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 27sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 29sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 31sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 33sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 35sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 38sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 40sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 42sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 44sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 46sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 48sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state PENDING.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 50sec out of 120sec.\n",
      "Running wait for in progress callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in non pending state DEQUEUED.\n",
      "Finished wait_for_in_progress polling stage.\n",
      "Starting wait_for_completion polling stage.\n",
      "Polling Job API. Wait time: 0sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state session_id='7f517cb7-80d3-436e-8a46-3044aaec4e2a' created_timestamp=1722469736 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1722469721, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1722469726, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 2sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state session_id='7f517cb7-80d3-436e-8a46-3044aaec4e2a' created_timestamp=1722469736 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1722469721, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1722469726, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 5sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state session_id='7f517cb7-80d3-436e-8a46-3044aaec4e2a' created_timestamp=1722469736 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1722469721, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1722469726, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 7sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state session_id='7f517cb7-80d3-436e-8a46-3044aaec4e2a' created_timestamp=1722469736 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1722469721, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1722469726, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 9sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state session_id='7f517cb7-80d3-436e-8a46-3044aaec4e2a' created_timestamp=1722469736 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1722469721, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1722469726, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 11sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state session_id='7f517cb7-80d3-436e-8a46-3044aaec4e2a' created_timestamp=1722469736 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1722469721, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1722469726, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 13sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state session_id='7f517cb7-80d3-436e-8a46-3044aaec4e2a' created_timestamp=1722469736 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1722469721, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1722469726, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 15sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in state session_id='7f517cb7-80d3-436e-8a46-3044aaec4e2a' created_timestamp=1722469736 username='parth' batch_id=None payload={'revalidate': False, 'record': {'outputs': [{'dataset_template_id': '10378.1/1926245', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904961'}], 'associations': {'modeller_id': '10378.1/1893843', 'requesting_organisation_id': '10378.1/1893860'}, 'start_time': 1722469721, 'inputs': [{'dataset_template_id': '10378.1/1905250', 'dataset_type': 'DATA_STORE', 'dataset_id': '10378.1/1904964'}], 'end_time': 1722469726, 'description': 'Standard Provena Model Run Example', 'workflow_template_id': '10378.1/1905251', 'display_name': 'Notebook Model Run Testing'}} job_type=<JobType.PROV_LODGE: 'PROV_LODGE'> job_sub_type=<JobSubType.MODEL_RUN_PROV_LODGE: 'MODEL_RUN_PROV_LODGE'> gsi_status='ok' status=<JobStatus.IN_PROGRESS: 'IN_PROGRESS'> info='Job has been dispatched to worker callback and is in progress.' result=None.\n",
      "Callback registered incomplete. Waiting for polling interval.\n",
      "Polling Job API. Wait time: 17sec out of 180sec.\n",
      "Running wait for completion callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a in completed state.\n",
      "Finished wait_for_completion polling stage.\n",
      "\n",
      "Current job status: JobStatus.SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the job to see if it's complete. We will do this by polling the job_api\n",
    "job_result = await client.job_api.await_successful_job_completion(session_id=model_run_register_result.session_id)\n",
    "\n",
    "while job_result.status != JobStatus.SUCCEEDED: # Keep polling on this cell till this turns to \"SUCCEEDED\"\n",
    "    \n",
    "    job_result = await client.job_api.await_successful_job_completion(session_id=model_run_register_result.session_id)\n",
    "    pprint(job_result.result)\n",
    "    pprint(job_result.job_type)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Current job status:\", job_result.status) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prov_json\": \"{\\\"prefix\\\": {\\\"default\\\": \\\"http://hdl.handle.net/\\\"}, \\\"activity\\\": {\\\"10378.1/1939260\\\": {\\\"model_run/10378.1/1939260\\\": true, \\\"item_category\\\": \\\"ACTIVITY\\\", \\\"item_subtype\\\": \\\"MODEL_RUN\\\"}}, \\\"entity\\\": {\\\"10378.1/1904964\\\": {\\\"model_run/10378.1/1939260\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"DATASET\\\"}, \\\"10378.1/1904961\\\": {\\\"model_run/10378.1/1939260\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"DATASET\\\"}, \\\"10378.1/1905251\\\": {\\\"model_run/10378.1/1939260\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"MODEL_RUN_WORKFLOW_TEMPLATE\\\", \\\"prov:type\\\": {\\\"$\\\": \\\"prov:Collection\\\", \\\"type\\\": \\\"prov:QUALIFIED_NAME\\\"}}, \\\"10378.1/1905250\\\": {\\\"model_run/10378.1/1939260\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"DATASET_TEMPLATE\\\"}, \\\"10378.1/1926245\\\": {\\\"model_run/10378.1/1939260\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"DATASET_TEMPLATE\\\"}, \\\"10378.1/1924630\\\": {\\\"model_run/10378.1/1939260\\\": true, \\\"item_category\\\": \\\"ENTITY\\\", \\\"item_subtype\\\": \\\"MODEL\\\"}}, \\\"agent\\\": {\\\"10378.1/1893843\\\": {\\\"model_run/10378.1/1939260\\\": true, \\\"item_category\\\": \\\"AGENT\\\", \\\"item_subtype\\\": \\\"PERSON\\\"}, \\\"10378.1/1893860\\\": {\\\"model_run/10378.1/1939260\\\": true, \\\"item_category\\\": \\\"AGENT\\\", \\\"item_subtype\\\": \\\"ORGANISATION\\\"}}, \\\"used\\\": {\\\"_:id1\\\": {\\\"prov:activity\\\": \\\"10378.1/1939260\\\", \\\"prov:entity\\\": \\\"10378.1/1904964\\\"}, \\\"_:id3\\\": {\\\"prov:activity\\\": \\\"10378.1/1939260\\\", \\\"prov:entity\\\": \\\"10378.1/1924630\\\"}, \\\"_:id4\\\": {\\\"prov:activity\\\": \\\"10378.1/1939260\\\", \\\"prov:entity\\\": \\\"10378.1/1905251\\\"}}, \\\"wasGeneratedBy\\\": {\\\"_:id2\\\": {\\\"prov:entity\\\": \\\"10378.1/1904961\\\", \\\"prov:activity\\\": \\\"10378.1/1939260\\\"}}, \\\"wasAssociatedWith\\\": {\\\"_:id5\\\": {\\\"prov:activity\\\": \\\"10378.1/1939260\\\", \\\"prov:agent\\\": \\\"10378.1/1893843\\\"}, \\\"_:id6\\\": {\\\"prov:activity\\\": \\\"10378.1/1939260\\\", \\\"prov:agent\\\": \\\"10378.1/1893860\\\"}}, \\\"wasAttributedTo\\\": {\\\"_:id7\\\": {\\\"prov:entity\\\": \\\"10378.1/1904961\\\", \\\"prov:agent\\\": \\\"10378.1/1893843\\\"}}, \\\"hadMember\\\": {\\\"_:id8\\\": {\\\"prov:collection\\\": \\\"10378.1/1905251\\\", \\\"prov:entity\\\": \\\"10378.1/1905250\\\"}, \\\"_:id10\\\": {\\\"prov:collection\\\": \\\"10378.1/1905251\\\", \\\"prov:entity\\\": \\\"10378.1/1926245\\\"}, \\\"_:id12\\\": {\\\"prov:collection\\\": \\\"10378.1/1905251\\\", \\\"prov:entity\\\": \\\"10378.1/1924630\\\"}}, \\\"wasInfluencedBy\\\": {\\\"_:id9\\\": {\\\"prov:influencee\\\": \\\"10378.1/1904964\\\", \\\"prov:influencer\\\": \\\"10378.1/1905250\\\"}, \\\"_:id11\\\": {\\\"prov:influencee\\\": \\\"10378.1/1904961\\\", \\\"prov:influencer\\\": \\\"10378.1/1926245\\\"}}}\",\n",
      "  \"id\": \"10378.1/1939260\",\n",
      "  \"record\": {\n",
      "    \"outputs\": [\n",
      "      {\n",
      "        \"dataset_template_id\": \"10378.1/1926245\",\n",
      "        \"dataset_type\": \"DATA_STORE\",\n",
      "        \"dataset_id\": \"10378.1/1904961\"\n",
      "      }\n",
      "    ],\n",
      "    \"associations\": {\n",
      "      \"modeller_id\": \"10378.1/1893843\",\n",
      "      \"requesting_organisation_id\": \"10378.1/1893860\"\n",
      "    },\n",
      "    \"start_time\": 1722469721,\n",
      "    \"inputs\": [\n",
      "      {\n",
      "        \"dataset_template_id\": \"10378.1/1905250\",\n",
      "        \"dataset_type\": \"DATA_STORE\",\n",
      "        \"dataset_id\": \"10378.1/1904964\"\n",
      "      }\n",
      "    ],\n",
      "    \"end_time\": 1722469726,\n",
      "    \"description\": \"Standard Provena Model Run Example\",\n",
      "    \"workflow_template_id\": \"10378.1/1905251\",\n",
      "    \"display_name\": \"Notebook Model Run Testing\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "method_two_record_info = job_result.result[\"record\"]\n",
    "pprint_json(method_two_record_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick and dirty visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/graphviz/backend/execute.py:78\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: PosixPath('dot')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m filepath \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresources/prov_graph\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m filepath\u001b[38;5;241m.\u001b[39mwrite_text(src, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mgraphviz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/graphviz/backend/rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 326\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/graphviz/backend/execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "import prov.model as pm\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "import pathlib\n",
    "\n",
    "# pull out json serialisation from prov api\n",
    "prov_serialisation = method_two_record_info[\"prov_json\"]\n",
    "\n",
    "# parse into prov document\n",
    "document = pm.ProvDocument.deserialize(source=None, content=prov_serialisation)\n",
    "\n",
    "# render into file\n",
    "dot = prov_to_dot(document)\n",
    "src = dot.to_string()\n",
    "filepath = pathlib.Path('resources/prov_graph')\n",
    "filepath.write_text(src, encoding='ascii')\n",
    "\n",
    "graphviz.render('dot', 'png', filepath).replace('\\\\', '/')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registering a dataset during model run registration\n",
    "\n",
    "Now let's perform method 1) i.e. register a dataset during automated model run provenance registration.\n",
    "\n",
    "Datasets require the following fields.\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"associations\": {\n",
    "    \"organisation_id\": \"string\"\n",
    "  },\n",
    "  \"dataset_info\": {\n",
    "    \"name\": \"string\",\n",
    "    \"description\": \"string\",\n",
    "    \"access_info\": {\n",
    "      \"reposited\": true,\n",
    "      \"uri\": \"string\",\n",
    "      \"description\": \"string\"\n",
    "    },\n",
    "    \"publisher_id\": \"string\",\n",
    "    \"created_date\": \"2023-06-06\",\n",
    "    \"published_date\": \"2023-06-06\",\n",
    "    \"license\": \"string\",\n",
    "    \"preferred_citation\": \"string\",\n",
    "    \"keywords\": [\n",
    "      \"string\"\n",
    "    ],\n",
    "    \"version\": \"string\"\n",
    "  },\n",
    "  \"approvals\": {\n",
    "    \"ethics_registration\": {\n",
    "      \"relevant\": false,\n",
    "      \"obtained\": false\n",
    "    },\n",
    "    \"ethics_access\": {\n",
    "      \"relevant\": false,\n",
    "      \"obtained\": false\n",
    "    },\n",
    "    \"indigenous_knowledge\": {\n",
    "      \"relevant\": false,\n",
    "      \"obtained\": false\n",
    "    },\n",
    "    \"export_controls\": {\n",
    "      \"relevant\": false,\n",
    "      \"obtained\": false\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minted new dataset successfully with handle 10378.1/1939263.\n"
     ]
    }
   ],
   "source": [
    "from ProvenaInterfaces.RegistryAPI import *\n",
    "from ProvenaInterfaces.DataStoreAPI import CollectionFormat\n",
    "\n",
    "default_license = \"https://gbrrestoration.github.io/rrap-mds-knowledge-hub/information-system/licenses.html#copyright-all-rights-reserved\"\n",
    "output_path = \"s3://example-bucket/test-data.csv\"\n",
    "\n",
    "dataset_payload = CollectionFormat(\n",
    "        associations= CollectionFormatAssociations(\n",
    "            organisation_id=config.associations.organisation,\n",
    "            data_custodian_id=config.associations.person,\n",
    "            point_of_contact=None\n",
    "        ),\n",
    "        approvals=CollectionFormatApprovals(\n",
    "            ethics_registration=DatasetEthicsRegistrationCheck(),\n",
    "            ethics_access=DatasetEthicsAccessCheck(),\n",
    "            indigenous_knowledge=IndigenousKnowledgeCheck(),\n",
    "            export_controls=ExportControls()\n",
    "        ), \n",
    "        dataset_info=CollectionFormatDatasetInfo(\n",
    "            name=\"Notebook Example workflow\",\n",
    "            description = \"Example workflow output (Standard Provena),Generated in automated provenance workflow registration. Externally reposited\", \n",
    "            access_info=AccessInfo(reposited=False, uri = output_path, description=\"The file is stored in the Example storage at the specified path\"),\n",
    "            publisher_id=config.associations.organisation,\n",
    "            created_date= CreatedDate(relevant = False, value = None),\n",
    "            published_date= PublishedDate(relevant = False, value = None),\n",
    "            license=default_license,\n",
    "            purpose = None, \n",
    "            rights_holder=None,\n",
    "            usage_limitations=None,\n",
    "            temporal_info=None,\n",
    "            formats=None,\n",
    "            keywords=[ \"JYI\",\"Example\"],\n",
    "            user_metadata=None,\n",
    "            version=None\n",
    "        )\n",
    "        \n",
    ")\n",
    "\n",
    "register_dataset = await client.datastore.mint_dataset(dataset_mint_info=dataset_payload)\n",
    "\n",
    "output_dataset_id = register_dataset.handle\n",
    "\n",
    "print(f\"Minted new dataset successfully with handle {output_dataset_id}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have dynamically generated an output dataset. \n",
    "\n",
    "We could choose to actually upload the files to this dataset using the dynamic s3 credential generation. \n",
    "\n",
    "However, in the NBIC workflows we just want to refer to the existing storage location - so we use the externally reposited option as above to specify this path. \n",
    "\n",
    "Now we can use the same model run payload, replacing the output dataset with the dynamically generated output above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model run\n"
     ]
    }
   ],
   "source": [
    "model_run_payload.outputs[0].dataset_id = output_dataset_id\n",
    "\n",
    "## Registering the model run \n",
    "payload = model_run_payload\n",
    "print(\"Registering model run\")\n",
    "model_run_register_response = await client.prov_api.register_model_run(model_run_payload=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting wait_for_entry_in_queue polling stage.\n",
      "Polling Job API. Wait time: 0sec out of 20sec.\n",
      "Running wait_for_entry_in_queue callback. Session ID: 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "200OK response for user fetch of 7f517cb7-80d3-436e-8a46-3044aaec4e2a.\n",
      "Finished wait_for_entry_in_queue polling stage.\n",
      "\n",
      "Current job status: JobStatus.SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the job to see if it's complete. We will do this by polling the job_api\n",
    "job_result = await client.job_api.await_successful_job_completion(session_id=model_run_register_result.session_id)\n",
    "\n",
    "while job_result.status != JobStatus.SUCCEEDED: # Keep polling on this cell till this turns to \"SUCCEEDED\"\n",
    "    \n",
    "    job_result = await client.job_api.await_successful_job_completion(session_id=model_run_register_result.session_id)\n",
    "    pprint(job_result.result)\n",
    "    pprint(job_result.job_type)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Current job status:\", job_result.status) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] \"dot\" not found in path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/pydot/core.py:1753\u001b[0m, in \u001b[0;36mDot.create\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1753\u001b[0m     stdout_data, stderr_data, process \u001b[38;5;241m=\u001b[39m \u001b[43mcall_graphviz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/pydot/core.py:133\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[0;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m program_with_args \u001b[38;5;241m=\u001b[39m [program] \u001b[38;5;241m+\u001b[39m arguments\n\u001b[0;32m--> 133\u001b[0m process \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogram_with_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworking_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m stdout_data, stderr_data \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m dot \u001b[38;5;241m=\u001b[39m prov_to_dot(document)\n\u001b[1;32m     11\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresources/prov_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mdot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m Image(name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/pydot/core.py:1587\u001b[0m, in \u001b[0;36mDot.__init__.<locals>.new_method\u001b[0;34m(path, f, prog, encoding)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_method\u001b[39m(path, f\u001b[38;5;241m=\u001b[39mfrmt, prog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Refer to docstring of method `write.`\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1587\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/pydot/core.py:1662\u001b[0m, in \u001b[0;36mDot.write\u001b[0;34m(self, path, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1660\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(s)\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1662\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1663\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1664\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(s)\n",
      "File \u001b[0;32m~/client_work/provena-example-notebooks/.venv/lib/python3.10/site-packages/pydot/core.py:1762\u001b[0m, in \u001b[0;36mDot.create\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1760\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(e\u001b[38;5;241m.\u001b[39margs)\n\u001b[1;32m   1761\u001b[0m     args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{prog}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found in path.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(prog\u001b[38;5;241m=\u001b[39mprog)\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1764\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path."
     ]
    }
   ],
   "source": [
    "method_one_record_info = job_result.result[\"record\"]\n",
    "\n",
    "# pull out json serialisation from prov api\n",
    "prov_serialisation = method_one_record_info[\"prov_json\"]\n",
    "\n",
    "# parse into prov document\n",
    "document = pm.ProvDocument.deserialize(source=None, content=prov_serialisation)\n",
    "\n",
    "# render into file\n",
    "dot = prov_to_dot(document)\n",
    "name = \"resources/prov_graph\"\n",
    "\n",
    "dot.write_png(name + '.png')\n",
    "Image(name + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating download and upload\n",
    "\n",
    "If, unlike in the above demonstration, we wanted to upload and download files, it is easy to use the Provena APIs to automate this process. \n",
    "\n",
    "The high level steps are:\n",
    "\n",
    "1. Identify or register the dataset\n",
    "2. Use the API to generate credentials to r/w to the dataset\n",
    "3. Use these credentials in your S3 client of choice to r/w data\n",
    "\n",
    "We will demonstrate these steps below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering dataset\n",
      "Minted new dataset successfully with handle 10378.1/1939266.\n"
     ]
    }
   ],
   "source": [
    "# First, let's register a dataset\n",
    "\n",
    "default_license = \"https://gbrrestoration.github.io/rrap-mds-knowledge-hub/information-system/licenses.html#copyright-all-rights-reserved\"\n",
    "dataset_payload = CollectionFormat(\n",
    "    associations=CollectionFormatAssociations(\n",
    "        organisation_id=config.associations.organisation,\n",
    "        data_custodian_id=None,\n",
    "        point_of_contact=None\n",
    "    ),\n",
    "    approvals=CollectionFormatApprovals(\n",
    "        ethics_registration=DatasetEthicsRegistrationCheck(\n",
    "            relevant=False,\n",
    "            obtained=False\n",
    "        ),\n",
    "        ethics_access=DatasetEthicsAccessCheck(\n",
    "            relevant=False,\n",
    "            obtained=False\n",
    "        ),\n",
    "        indigenous_knowledge=IndigenousKnowledgeCheck(\n",
    "            relevant=False,\n",
    "            obtained=False\n",
    "        ),\n",
    "        export_controls=ExportControls(\n",
    "            relevant=False,\n",
    "            obtained=False\n",
    "        )\n",
    "    ),\n",
    "    dataset_info=CollectionFormatDatasetInfo(\n",
    "        name=\"Demonstration upload download dataset\",\n",
    "        description=\"This dataset is used to demonstrate automated upload and download of files.\",\n",
    "        access_info=AccessInfo(\n",
    "            reposited=True,\n",
    "            uri=None,\n",
    "            description=None\n",
    "        ),\n",
    "        publisher_id=config.associations.organisation,\n",
    "        created_date=CreatedDate(relevant = True, value = date(year=2023, month=6, day=6)),\n",
    "        published_date=PublishedDate(relevant = True, value = date(year=2023, month=6, day=6)),\n",
    "        license=default_license,\n",
    "        purpose=None,\n",
    "        rights_holder=None,\n",
    "        usage_limitations=None,\n",
    "        temporal_info=None,\n",
    "        formats=None,\n",
    "        keywords=[\"JYI\", \"Example\"],\n",
    "        user_metadata=None,\n",
    "        version=None\n",
    "    )\n",
    ")\n",
    "\n",
    "# send off request\n",
    "print(\"Registering dataset\")\n",
    "dataset_mint_response = await client.datastore.mint_dataset(dataset_mint_info=dataset_payload)\n",
    "\n",
    "dataset_id = dataset_mint_response.handle\n",
    "\n",
    "print(f\"Minted new dataset successfully with handle {dataset_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: restored-dev-dev-rrap-storage-bucket-11102022-11102022, Path: datasets/10378-1-1939266/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_id = dataset_mint_response.handle\n",
    "bucket_name = dataset_mint_response.s3_location.bucket_name\n",
    "bucket_path = dataset_mint_response.s3_location.path\n",
    "\n",
    "print(f\"Name: {bucket_name}, Path: {bucket_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can use the built in download and upload methods, or we can spin up our own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files...\n",
      "\n",
      "Resulting files...\n",
      "photo.jpg\n",
      "metadata.json\n",
      "document.txt\n",
      "data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "with TemporaryDirectory() as upload_dir: \n",
    "    # Assume we have these files in our local system initially\n",
    "    local_files = ['document.txt', 'photo.jpg', 'data.csv']\n",
    "\n",
    "    # Create the files locally to simulate existing files\n",
    "    for file_name in local_files:\n",
    "        with open(os.path.join(upload_dir, file_name), 'w') as file:\n",
    "            file.write(f\"This is the content of {file_name}.\")\n",
    "    \n",
    "    # Upload all files created in temp directory.\n",
    "    await client.datastore.io.upload_all_files(source_directory=upload_dir, dataset_id=dataset_id)        \n",
    " \n",
    "    # Download and check into temp dir\n",
    "    with TemporaryDirectory() as download_dir: \n",
    "        print(\"Downloading files...\")\n",
    "        await client.datastore.io.download_all_files(destination_directory=download_dir, dataset_id=dataset_id)\n",
    "        print()\n",
    "        \n",
    "        print(\"Resulting files...\")\n",
    "        files = os.listdir(download_dir)\n",
    "        for f in files:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading files using S3 SDK...\n",
      "\n",
      "Downloading files...\n",
      "\n",
      "Resulting files...\n",
      "photo.jpg\n",
      "metadata.json\n",
      "document.txt\n",
      "data2.csv\n",
      "data.csv\n",
      "photo2.jpg\n",
      "document2.txt\n"
     ]
    }
   ],
   "source": [
    "from ProvenaInterfaces.DataStoreAPI import CredentialsRequest\n",
    "\n",
    "# Or we can use AWS S3 SDK directly, for example\n",
    "import boto3 \n",
    "\n",
    "# Generate Write Credentials through the client library\n",
    "credential_request = CredentialsRequest(dataset_id=dataset_id, console_session_required=False)\n",
    "creds = await client.datastore.generate_write_access_credentials(credentials=credential_request)\n",
    "\n",
    "# This will mess up constructor - remove for now\n",
    "filtered_creds = {k : v for k,v in creds.credentials if k != \"expiry\"}\n",
    "\n",
    "# create a session\n",
    "s3 = boto3.client('s3', **filtered_creds)\n",
    "\n",
    "# now lets demonstrate uploading/downloading directly\n",
    "with TemporaryDirectory() as upload_dir: \n",
    "    # Assume we have these files in our local system initially\n",
    "    local_files = ['document2.txt', 'photo2.jpg', 'data2.csv']\n",
    "\n",
    "    # Create the files locally to simulate existing files\n",
    "    for file_name in local_files:\n",
    "        with open(os.path.join(upload_dir, file_name), 'w') as file:\n",
    "            file.write(f\"This is the content of {file_name}.\")\n",
    "\n",
    "    # Upload all files in dir\n",
    "    print(\"Uploading files using S3 SDK...\")\n",
    "    for f in local_files:\n",
    "        s3.upload_file(os.path.join(upload_dir, f),bucket_name,bucket_path +  f)\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    # Download and check into temp dir\n",
    "    with TemporaryDirectory() as download_dir: \n",
    "        print(\"Downloading files...\")\n",
    "        await client.datastore.io.download_all_files(destination_directory=download_dir, dataset_id=dataset_id)\n",
    "        print()\n",
    "        \n",
    "        print(\"Resulting files...\")\n",
    "        files = os.listdir(download_dir)\n",
    "        for f in files:\n",
    "            print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
